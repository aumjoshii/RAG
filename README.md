# Ragline ‚Äî Local RAG System (Free, Offline)

A fully functional Retrieval-Augmented Generation (RAG) app that runs locally with:
- **.NET API** (upload, index, ask)
- **Local embeddings** via a Python FastAPI microservice (Sentence-Transformers)
- **Local LLM** via Ollama
- **SQLite persistence** (restart-safe)
- Simple UI to upload files, ask questions, and view sources

## Features
- Upload **.txt** (and optionally .pdf if enabled) and index into a vector store (SQLite)
- Ask questions and get:
  - Answer generated by a local LLM (Ollama)
  - Top-k supporting source chunks with similarity scores
- List indexed documents in UI

## Tech Stack
- Backend: **.NET 8 Minimal API**
- Embeddings: **sentence-transformers/all-MiniLM-L6-v2**
- LLM: **Ollama** (default: `llama3.2:3b`)
- Storage: **SQLite** (`rag.db`), local uploads folder

## Prerequisites
- .NET 8 SDK
- Python 3
- Ollama

## Quick Start (Recommended)
From repo root:

### 1) Start everything
```bash
./run.sh

üß† How Ragline Works (End-to-End)

Ragline is a fully local Retrieval-Augmented Generation (RAG) system designed to produce grounded, explainable answers from user-provided documents.

Pipeline:

Document ingestion

Accepts .pdf and .txt files

PDF text extracted page-by-page using PdfPig

Text is cleaned (normalized whitespace, line breaks removed)

Chunking

Documents are split into overlapping chunks

Default: 500 characters with 100 character overlap

Overlap preserves context across boundaries

Embeddings

Each chunk is embedded using a local Python FastAPI service

Model: sentence-transformers/all-MiniLM-L6-v2

Embeddings are stored in SQLite alongside chunk metadata

Question answering

User question is embedded

Top-K similar chunks are retrieved using cosine similarity

Retrieved context is sent to a local LLM (Ollama)

The model answers only using retrieved sources

‚úÖ Why Ragline‚Äôs Answers Are Trustworthy

Ragline is designed to avoid hallucinations:

Answers are generated only from retrieved document chunks

Each answer includes:

Source document name

Page number

Chunk index

Similarity score

Similarity Thresholding

If the highest similarity score is low, Ragline:

Warns the model internally

Displays a ‚Äúweak match‚Äù confidence in the UI

This prevents confident answers when retrieval is poor

Confidence score interpretation:

< 0.15 ‚Üí weak / likely unrelated

0.18 ‚Äì 0.25 ‚Üí partial relevance

0.25 ‚Äì 0.35 ‚Üí strong semantic match

> 0.35 ‚Üí very strong grounding

üìä UI Transparency

The UI exposes retrieval quality directly:

Confidence badge per answer

Visual indicators for weak vs strong matches

Expandable list of retrieved source chunks

This makes the system explainable and debuggable, not a black box.

üê≥ Running with Docker

From the repository root:

docker compose up --build


Services started:

.NET API (document ingestion + RAG logic)

Embeddings service (Python + sentence-transformers)

Ollama (local LLM inference)

üõ† Common Issues & Fixes

Ollama returns 404

docker compose exec ollama ollama pull llama3.2:3b


Low confidence answers

Re-upload documents after changing chunk size

Ensure text extraction is clean

Ask questions closely related to document content

Docker daemon not running

Start Docker Desktop and retry

üéØ Project Goals

Ragline is built to demonstrate:

Practical RAG architecture

Grounded generation with explainability

Local, cost-free AI systems

Production-minded engineering choices
